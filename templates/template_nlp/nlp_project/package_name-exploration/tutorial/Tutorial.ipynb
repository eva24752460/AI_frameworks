{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prerequisites :**\n",
    "- Download the fasttext embedding matrix: https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fr.300.vec.gz\n",
    "\n",
    "- **Launch this notebook with the kernel of your virtual environment** (used in the last part of this notebook). In order to create a kernel linked to your virtual environment : `python -m ipykernel install --user --name=name_of_the_kernel` (when your virtual environment is activated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Understand how the nlp template works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Why use the NLP template?**\n",
    "\n",
    "The NLP (natural language processing) template automatically generates a NLP project including the more mainstream models and facilitating their industrialization.\n",
    "\n",
    "The generated project can be used for **classification** tasks on text data. Of course, you have to adapt it to your particular use case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Structure of the generated project**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".   \n",
    "├── <span style=\"color:darkred\">{{package_name}}</span>  **# The package** <br>\n",
    "│   ├── <span style=\"color:darkred\">models_training</span> **# Folder containing all the modules pertaining to the models** <br>\n",
    "│   ├── <span style=\"color:darkred\">monitoring</span> **# Folder containing all the modules pertaining to the explainers and MLflow** <br>\n",
    "│   ├── <span style=\"color:darkred\">preprocessing</span> **# Folder containing all the modules pertaining to preprocessing** <br>\n",
    "├── <span style=\"color:darkred\">{{package_name}}-data</span>   &emsp;&emsp;&emsp;&emsp;**# Folder containing all the datasets** <br>\n",
    "├── <span style=\"color:darkred\">{{package_name}}-exploration</span>   &emsp;&emsp;&emsp;&emsp;**# Folder containing this tutorial and which should contain all your experiments and explorations** <br> \n",
    "├── <span style=\"color:darkred\">{{package_name}}-models</span>   &emsp;&emsp;&emsp;&emsp;**# Folder containing all the models generated** <br>\n",
    "├── <span style=\"color:darkred\">{{package_name}}-ressources</span>   &emsp;&emsp;&emsp;&emsp;**# Folder containing some ressources such as the instructions to upload a model** <br>\n",
    "├── <span style=\"color:darkred\">{{package_name}}-scripts</span>   &emsp;&emsp;&emsp;&emsp;**# Folder containing examples script to preprocess data, train models, predict and use a demonstrator** <br>\n",
    "│   ├── <span style=\"color:darkred\">active_learning</span> **# Folder containing an example of active learning** <br>\n",
    "│   ├── <span style=\"color:darkred\">utils</span> **# Folder containing scripts to preprocess data** <br>\n",
    "│   ├── <span style=\"color:darkred\">utils_torch</span> **# Folder containing scripts to put data in pytorch format** <br>\n",
    "├── <span style=\"color:darkred\">{{package_name}}-transformers</span>   &emsp;&emsp;&emsp;&emsp;**# Folder containing the pytorch transformers** <br>\n",
    "├── <span style=\"color:darkred\">{{package_name}}.egg-info</span>   &emsp;&emsp;&emsp;&emsp;**# Folder containing various data on the package** <br>\n",
    "├── <span style=\"color:darkred\">tests</span>   &emsp;&emsp;&emsp;&emsp;**# Folder containing all the unit tests** <br>\n",
    "├── .gitignore <br>\n",
    "├── Makefile <br>\n",
    "├── README.md    <br>\n",
    "├── requirements.txt    <br>\n",
    "└── setup.py   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**General principles on the generated packages**\n",
    "\n",
    "- Data must be saved in the `{{package_name}}-data` folder<br>\n",
    "<br>\n",
    "- Trained models will automatically be saved in the `{{package_name}}-models` folder<br>\n",
    "<br>\n",
    "- Be aware that all the functions/methods for writing/reading files uses these two folders as base. Thus when a script has an argument for the path of a file/model, the given path should be **relative** to the `{{package_name}}-data`/`{{package_name}}-models` folders.<br>\n",
    "<br>\n",
    "- The provided scripts in `{{package_name}}-scripts` are given as example. You can use them to help you develop but their use is not required. The package is more useful by providing the functions contained in utils, preprocessing and in the models' classes<br>\n",
    "<br>\n",
    "- The file `preprocess.py` contains the various preprocessing pipelines used in this package. This file contains a dictionary of pipelines. It will be used to create datasets (one for each preprocessing pipeline). Be very careful when you modify a pipeline because, the already trained model won't be retrocompatible with it. It is generally advised to create a new pipeline.<br>\n",
    "<br>\n",
    "- You can use this package for mono-label and multi-labels tasks (`multi_label` argument in models' classes)<br>\n",
    "<br>\n",
    "- The modelling part is structured as follows :\n",
    "    - ModelClass: main class taking care of saving data and metrics (among other)\n",
    "    - ModelPipeline: child class of ModelClass managing all models related to a sklearn pipeline\n",
    "    - ModelKeras: child class of ModelClass managing all models using Keras\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Use the template to train your first model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For that purpose, we will use a dataset containing popular video games from the website jeuxvideo.com\n",
    "\n",
    "This dataset contains a description of the games and their type (Action, RPG, etc.). The goal will be to predict the type of the game from its description.\n",
    "\n",
    "Note that the dataset is in french but it is not necessary to understand french to follow this tutorial.\n",
    "\n",
    "Note : in the following exercises, the datasets are .csv with `;` as separator and `utf-8` as encoding. These are the default values of the generated project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Exercice 1**</span>\n",
    "\n",
    "Goal:\n",
    "\n",
    "-Split a dataset in train / valid / test\n",
    "\n",
    "TODO:\n",
    "- Use the script `utils/0_split_train_valid_test.py` on the dataset `{{package_name}}-data/dataset_jvc.csv`\n",
    "- We want a 'random' split but **with a random seed set to 42** (in order to always reproduce the same results)\n",
    "- We use default splitting ratio (60/20/20)\n",
    "\n",
    "Help:\n",
    "- The file `utils/0_split_train_valid_test.py` splits a dataset in 3 .csv files:\n",
    "    - {fichier}_train.csv : the training dataset\n",
    "    - {fichier}_valid.csv : the validation dataset\n",
    "    - {fichier}_test.csv : the test dataset\n",
    "- You can specify the type of split : random, stratified or hierarchical (here, use random)\n",
    "- Reminder: the path to the file to process is relative to `{{package_name}}-data`\n",
    "- To get the possible arguments of the script : `python 0_split_train_valid_test.py --help`\n",
    "- Don't forget to activate your virtual environment ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 1** :  Validation\n",
    "\n",
    "~ Run the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlp\n",
    "nlp.test_exercice_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 1** :  Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlp\n",
    "nlp.get_exercice_1_solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Exercice 2**</span>\n",
    "\n",
    "Goal:\n",
    "\n",
    "-Obtain a random sample of the file `dataset_jvc_train.csv` (n=10) (we won't use it after, this exercise is just here to show what can be done)\n",
    "\n",
    "TODO:\n",
    "- Use the script `utils/0_create_samples.py` on the dataset `{{package_name}}-data/dataset_jvc.csv`\n",
    "- We want a sample of 10 lines\n",
    "\n",
    "Help:\n",
    "- The file `utils/0_create_samples.py` samples a dataset\n",
    "- To get the possible arguments of the script : `python 0_create_samples.py --help`\n",
    "- Don't forget to activate your virtual environment ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 2** :  Validation\n",
    "\n",
    "~ Run the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlp\n",
    "nlp.test_exercice_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 2** :  Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlp\n",
    "nlp.get_exercice_2_solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Exercice 3**</span>\n",
    "\n",
    "Goal:\n",
    "\n",
    "- Apply the default preprocessing to `dataset_jvc_train.csv`\n",
    "\n",
    "TODO:\n",
    "- Use the script `1_preprocess_data.py` on the dataset `{{package_name}}-data/dataset_jvc_train.csv` to apply the default pipeline (`preprocess_P1`)\n",
    "- The preprocessing must be done on the `description` column\n",
    "\n",
    "Help:\n",
    "- The file `1_preprocess_data.py` applies a preprocessing pipeline **to one column of one or several .csv files**\n",
    "- Without the argument `preprocessing`, this python script creates as many files as there are **pipelines registered in `preprocessing/preprocess.py`**\n",
    "- It works as follows:<br>\n",
    "    - In `preprocessing/preprocess.py`: <br>\n",
    "        - There is a dictionary of function (`get_preprocessors_dict`): key: str -> function <br>\n",
    "            - /!\\ Don't remove the element 'no_preprocess': lambda x: x /!\\ <br>\n",
    "        - There are preprocessing functions (usually from words_n_fun pipelines) <br>\n",
    "    - In `1_preprocess_data.py` :<br>\n",
    "        - Reads the dictionary of `preprocessing/preprocess.py` <br>\n",
    "        - For each 'key' of the dictionary (except `no_preprocess`):<br>\n",
    "            - Get the associated preprocessing function\n",
    "            - Load data\n",
    "            - Create a column `preprocessed_text` -> apply the preprocessing function\n",
    "            - Save the result -> {file_name}_{key}.csv <br>\n",
    "- To get the possible arguments of the script : `python 1_preprocess_data.py --help`\n",
    "- Don't forget to activate your virtual environment ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 3** :  Validation\n",
    "\n",
    "~ Run the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlp\n",
    "nlp.test_exercice_3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 3** :  Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlp\n",
    "nlp.get_exercice_3_solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Exercice 4**</span>\n",
    "\n",
    "Goal:\n",
    "\n",
    "- Apply a \"custom\" preprocess to `dataset_jvc_train.csv` and `dataset_jvc_valid.csv`\n",
    "\n",
    "TODO:\n",
    "- Add a new preprocessing pipeline `preprocess_P2` in `preprocessing/preprocess.py`\n",
    "\n",
    "- '''# pipeline to use <br>\n",
    "pipeline = ['remove_non_string', 'get_true_spaces', 'remove_punct', 'to_lower','remove_stopwords', 'trim_string', 'remove_leading_and_ending_spaces']\n",
    "'''\n",
    "- Use the script `1_preprocess_data.py` to apply the new pipeline `preprocess_P2`\n",
    "- The preprocessing must be done on the `description` column\n",
    "\n",
    "Help:\n",
    "- You have to create a new preprocessing in `preprocessing/preprocess.py` and add it to the dictionary of `get_preprocessors_dict()`\n",
    "- Don't forget to activate your virtual environment ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 4** :  Validation\n",
    "\n",
    "~ Run the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlp\n",
    "nlp.test_exercice_4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 4** :  Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlp\n",
    "nlp.get_exercice_4_solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">**Exercice 5**</span>\n",
    "\n",
    "Goal:\n",
    "\n",
    "- Use the script `2_training.py` to train a mono-label TD-IDF + SVM model to predict the 'RPG' category\n",
    "\n",
    "- Training dataset : `dataset_jvc_train_preprocess_P2.csv`\n",
    "\n",
    "- Validation dataset : `dataset_jvc_valid_preprocess_P2.csv`\n",
    "\n",
    "TODO:\n",
    "- Use the script `python 2_training.py` with the proper arguments\n",
    "\n",
    "- We want to train on the column `preprocessed_text`, result of the preprocessing on the `description` column\n",
    "\n",
    "- We want to predict the `RPG` column\n",
    "\n",
    "Help:\n",
    "- The script `2_training.py` trains a model on a dataset\n",
    "- It works as follows:<br>\n",
    "    - Read a train .csv file as input <br>\n",
    "        - If a validation file is given, it will use it <br>\n",
    "        - Otherwise, split the train dataset in two parts (train/validation) <br>\n",
    "    - Manage `y_col` argument: <br>\n",
    "        - If there is only one value, training in mono-label mode <br>\n",
    "        - If several values, training in multi-labels mode <br>\n",
    "    - **Manual modifications of the script**: <br>\n",
    "        - **To change the model used** -> you have to comment/uncomment/modify the code in the \"training\" part (not necessary for this exercise) <br>\n",
    "        - **To load datasets** -> if a dataset is not in the right format, you have to adapt the loading part (not necessary for this exercise) <br>\n",
    "    - Optionnal argument (no need to use them in this exercise): <br>\n",
    "        - `min_rows` minimal number of lines necessary to handle a class (default 0)\n",
    "        - `iter_for_keras` : Number of model iteration if it is a Keras model (experimental) <br>\n",
    "        - `level_save` : level of save <br>:\n",
    "            - `HIGH` : everything is saved (model, plots, predictions, etc.) <br>\n",
    "            - `MEDIUM` : the predictions are not saved\n",
    "            - `LOW` : we don't save models nor plots (be careful, you can't re-use the model)<br> \n",
    "- To get the possible arguments of the script : `python 2_training.py --help`\n",
    "- Don't forget to activate your virtual environment ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 5** :  Validation\n",
    "\n",
    "~ Manual validation\n",
    "\n",
    "After having executed the script `2_training.py`, you should see logs similar to this :\n",
    "\n",
    "<img src=\"images/model1.png\">\n",
    "\n",
    "With the default TF-IDF, you can see that the model overfits on the train dataset <br>\n",
    "\n",
    "A new model was created in the folder `{{package_name}}-models`. It contains the save of the model, results, statistics and plots.\n",
    "\n",
    "<img src=\"images/model1_path.png\">\n",
    "\n",
    "Details:\n",
    "- `plots/` : Folder containing the plots (here confusion matrices) <br>\n",
    "- `acc_train@​0.xx` : Empty file. The value after @ indicates the accuracy of the model on the train dataset <br>\n",
    "- `acc_valid@​0.xx` : Empty file. The value after @ indicates the accuracy of the model on the validation dataset <br>\n",
    "- `configurations.json` : **Configurations used by the model**. Mandatory yo re-use a model <br>\n",
    "- `f1_train@​0.xx.csv` : Statistics per class on the train dataset. The value after @ indicates the weighted f1-score <br>\n",
    "- `f1_valid@​0.xx.csv` : Statistics per class on the validation dataset. The value after @ indicates the weighted f1-score <br>\n",
    "- `model_{name_of_the_model}.pkl` : Pickle of the class of the trained model\n",
    "- `{name_of_the_type_of_model}_standalone.pkl` : Pickle of the model. Standalone version (ie. no need for the generated package, for example sklearn model) <br>\n",
    "- `model_upload_instructions.md` : Instructions to upload a model to use it (needs to be customized)\n",
    "- `predictions_train.csv` : Prediction on the train dataset. Wrong predictions first <br>\n",
    "- `predictions_valid.csv` : Prediction on the validation dataset. Wrong predictions first <br>\n",
    "- `proprietes.json` : Configuration file for uploading the model. Not useful for this tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercice 5** :  Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlp\n",
    "nlp.get_exercice_5_solution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Venv_template_nlp",
   "language": "python",
   "name": "venv_template_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
