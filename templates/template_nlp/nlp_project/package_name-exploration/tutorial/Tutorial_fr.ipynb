{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from {{package_name}} import utils\n",
    "from {{package_name}}.models_training import utils_models\n",
    "from {{package_name}}.models_training.model_tfidf_svm import ModelTfidfSvm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_sentence(text: str, min_sentence_size: int, min_sentence_word: int) -> List[str]:\n",
    "    '''Transforms a text in sentences.\n",
    "\n",
    "    Args:\n",
    "        text (str) : The text to cut in sentences\n",
    "        min_sentence_size (int) : The minimal number of characters in a sentence for it to be considered a sentence\n",
    "        min_sentence_word (int) : The minimal number of words in a sentence for it to be considered a sentence\n",
    "    Returns:\n",
    "        list : A list of sentence\n",
    "    \n",
    "    '''\n",
    "    text = re.sub(r'\\s',' ', text)\n",
    "    # Changes all 'strong' punctuations to a period\n",
    "    text = re.sub(r'\\!', r'.', text)\n",
    "    text = re.sub(r'\\?', r'.', text)\n",
    "    text = re.sub(r'\\;', r'.', text)\n",
    "    # Get rid of superfluous whitespaces\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    list_sentences = text.split('.')\n",
    "    list_sentences = [sentence for sentence in list_sentences if len(sentence) >= min_sentence_size]\n",
    "    list_sentences = [sentence for sentence in list_sentences if len(sentence.split(' ')) >= min_sentence_word]\n",
    "    return list_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On charge les textes\n",
    "data_path = utils.get_data_path()\n",
    "df_texts = pd.read_csv(os.path.join(data_path, 'texts.csv'), sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_sentence_size = 50\n",
    "min_sentence_word = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On constitue les phrases\n",
    "list_phrases = []\n",
    "for index, row in df_texts.iterrows():\n",
    "    text = row['text']\n",
    "    author = row['author']\n",
    "    book = row['book']\n",
    "    sentences = text_to_sentence(text, min_sentence_size, min_sentence_word)\n",
    "    list_phrases = list_phrases+[(sentence, author, book) for sentence in sentences]\n",
    "df_phrases = pd.DataFrame(list_phrases, columns=['sentence', 'author', 'book'])\n",
    "set_author = set(df_texts['author'])\n",
    "\n",
    "# On regarde la répartition des phrases par auteur\n",
    "df_phrases.value_counts('author')/len(df_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On mélange le dataset\n",
    "df_to_split = df_phrases.sample(frac=1)\n",
    "\n",
    "# On sélectionne un livre par auteur qui sera dans l'ensemble de validation\n",
    "dict_books_to_valid = {}\n",
    "for author in set_author:\n",
    "    set_books = set(df_texts[df_texts['author']==author]['book'])\n",
    "    dict_books_to_valid[author] = random.sample(set_books, k=1)[0]\n",
    "df_valid = df_to_split[df_to_split['book'].isin(dict_books_to_valid.values())].copy()\n",
    "\n",
    "# On sélectionne un autre livre par auteur qui sera dans l'ensemble de test\n",
    "dict_books_to_test = {}\n",
    "for author in set_author:\n",
    "    set_books = set(df_texts[df_texts['author']==author]['book'])\n",
    "    set_books.remove(dict_books_to_valid[author])\n",
    "    dict_books_to_test[author] = random.sample(set_books, k=1)[0]\n",
    "df_test = df_to_split[df_to_split['book'].isin(dict_books_to_test.values())].copy()\n",
    "\n",
    "# Tout le reste est dans l'ensemble d'entrainement\n",
    "df_train = df_to_split.copy()\n",
    "df_train = df_train[~df_train['book'].isin(dict_books_to_valid.values())]\n",
    "df_train = df_train[~df_train['book'].isin(dict_books_to_test.values())]\n",
    "\n",
    "# On sauvegarde les datasets\n",
    "utils.to_csv(df_train, os.path.join(data_path, 'dataset_texts_train.csv'))\n",
    "utils.to_csv(df_valid, os.path.join(data_path, 'dataset_texts_valid.csv'))\n",
    "utils.to_csv(df_test, os.path.join(data_path, 'dataset_texts_test.csv'))\n",
    "\n",
    "# On regarde la répartition des phrases par auteur dans l'ensemble d'entrainement\n",
    "df_train.value_counts('author')/len(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On fait un léger tuning\n",
    "dict_result = {}\n",
    "count =0\n",
    "for ngram_range in [(1, 1), (1, 2)]:\n",
    "    for C in [0.1, 0.5, 1, 2]:\n",
    "        model = ModelTfidfSvm(tfidf_params = {'ngram_range': ngram_range}, svc_params={'C':C})\n",
    "        model.fit(df_train['sentence'], df_train['author'], x_valid=df_valid['sentence'], y_valid=df_valid['author'])\n",
    "        df_train['pred'] = model.predict(df_train['sentence'])\n",
    "        df_valid['pred'] = model.predict(df_valid['sentence'])\n",
    "        score_train = f1_score(df_train['author'], df_train['pred'], average='macro')\n",
    "        score_val = f1_score(df_valid['author'], df_valid['pred'], average='macro')\n",
    "        dict_tmp = {'score_train':round(score_train, 5), 'score_val':round(score_val, 5), 'ngram_range':ngram_range, 'C':C}\n",
    "        dict_result[count] = dict_tmp.copy()\n",
    "        count += 1\n",
    "        print(dict_tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On sélectionne le meilleur modèle\n",
    "model = ModelTfidfSvm(tfidf_params = {'ngram_range': (1, 2)}, svc_params={'C':1})\n",
    "model.fit(df_train['sentence'], df_train['author'], x_valid=df_valid['sentence'], y_valid=df_valid['author'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On prédit sur le test\n",
    "df_test['pred'] = model.predict(df_test['sentence'])\n",
    "score = f1_score(df_test['author'], df_test['pred'], average='macro')\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_author(text: str, model, min_sentence_size: int, min_sentence_word: int, perc_sample: float=1.0) -> Tuple[str, dict, int]:\n",
    "    '''Predicts the author of a text.\n",
    "    \n",
    "    Args:\n",
    "        text (str) : The text whose author we want to predict\n",
    "        min_sentence_size (int) : The minimal number of characters in a sentence for it to be considered a sentence\n",
    "        min_sentence_word (int) : The minimal number of words in a sentence for it to be considered a sentence\n",
    "        perc_sample (float): The percentage of sentence of the text we consider\n",
    "    Returns:\n",
    "        tuple :\n",
    "            str : The predicted author\n",
    "            dict : The percentage of sentences attributed to each author\n",
    "            int : The number of sentences in the text\n",
    "    '''\n",
    "    # Cut the text in sentences\n",
    "    sentences = text_to_sentence(text, min_sentence_size, min_sentence_word)\n",
    "    sentences = random.sample(sentences, k=int(perc_sample*len(sentences)))\n",
    "    # For each sentence, predict an author. Gives the number of sentences predicted for each author\n",
    "    counter = dict(Counter(list(model.predict(sentences))))\n",
    "    # The author with the highest number of sentences\n",
    "    author = max(counter, key=counter.get)\n",
    "    # Calculates a percentage of sentences instead of raw numbers\n",
    "    count_sentences = [(key, round(value/len(sentences), 3)) for key, value in counter.items()]\n",
    "    count_sentences = sorted(count_sentences, key=lambda x:x[1], reverse=True)\n",
    "    return author, count_sentences, len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_result = {}\n",
    "# Pour chaque auteur\n",
    "for author, book in dict_books_to_test.items():\n",
    "    # Récupère le texte\n",
    "    row = df_texts[df_texts['book']==book]\n",
    "    text = row.iloc[0]['text']\n",
    "    length = len(text)\n",
    "    # Prédit l'auteur\n",
    "    prediction, counter, nb_sentences = predict_author(text, model, min_sentence_size, min_sentence_word, perc_sample=1.0)\n",
    "    # Enregistre le résultat\n",
    "    dict_result[author] = {'prediction': prediction, 'counter': counter, 'length': len(text), 'nb_sentences':nb_sentences}\n",
    "# Vérifie si le résultat est bon pour tous les auteurs\n",
    "print({key==value['prediction'] for key, value in dict_result.items()})\n",
    "# Montre les résultats\n",
    "dict_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Venv_template_nlp",
   "language": "python",
   "name": "venv_template_nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
